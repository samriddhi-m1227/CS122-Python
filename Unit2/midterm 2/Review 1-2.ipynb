{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMeHk+B6rq2SvWdAvUk8S2J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"FbEnDn98U3Y5"},"outputs":[],"source":["# Review of Intro to Data Wrangling 1 and 2"]},{"cell_type":"markdown","source":["1. Data Wrangling"],"metadata":{"id":"U6s1NwJI41WL"}},{"cell_type":"code","source":["#creating a dataframe:\n","\n","\"\"\"\n","\n","data={\n","  'review_id':[1,2,3],\n","  'movie_name': ['hello, 'flower', 'top'],\n","  etcc\n","}\n","\n","\"\"\""],"metadata":{"id":"Fpwis1n99tCU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#overview:\n","\"\"\"\n","cleaning: removing errors or inconsistencies\n","transforming: chnaging dtypes, values or structures\n","filtering: selecting relevant data points\n","feature engineeering: creating new feautures from existing data\n","\n","\n","\"\"\"\n","\n","#libraries\n","\"\"\"\n","DF: table(rows and columns)\n","Series: one dim array with labeled indicies\n","\n","Pandas: simplifies data prep:\n","read csv\n","df.dropna()\n","df.fillna()\n","df.groupby() for agg\n","\n","\n","NumPy: handling numeric data\n","- np.array() create arrays\n","- np.mean(), np.median() ..\n","\n","\n","Matplotlib and seaborn: data visualization\n","- stst visualizations\n","\n","Regex( re text analysis)\n","- extract patterns from text data\n","\n","\"\"\"\n","\n"],"metadata":{"id":"Y7bnD95qVNkJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# missing data\n","\n","\"\"\"\n","data=pd.read_csv(..)\n","\n","(data.shape) gives rows and columns\n","data.isnull().sum() shows missing count\n","\n","\n","data = data.dropna()\n","data=data.dropna(subset=['col1', 'col2']) drops missing rows from those columns\n","data2=data.dropna(axis=1) axis 0 is rows (defualt), 1 is columns\n","\n","\n","#Threshold\n","thresh=data.dropna(thresh=10) #must have atleast 10 filled vals, otherwise dropped\n","thresh=data.dropna(thresh=10, axis=1) --> for columns\n","\n","#Input missing data\n","\n","data_filled=data[['salary', 'bonus', 'age']].fillna(0)\n","data_filled=data.fillna({'First Name': 'Unknown', 'Date_Joined': 'Unknown'})\n","\n","numeric=data.select_dtypes(exclude='object').columns #gets only numeric\n","data_mean=data.fillna(data[numeric]).mean())\n","\n","data=data.fillna({'Age': data['Age'].mean(), 'height': data['height'].mean()})\n","\n","\"\"\"\n","#Forward and Backward fill:\n","\"\"\"\n","data_fill=data.ffil() #takes PRECEDING (top to bottom)\n","data_fill=data.bfill() #takes SUCCEDEING val (bottom to top)\n"," can apply axis to this ^\n"," axis=0 is rows (vertical)\n"," axis=1 is cols (horizontal)\n","\n","\"\"\"\n","#Removing duplicates\n","\"\"\"\n","data=data.dop_duplicates()\n","data=data.drop_duplicates(subset=['Salary']) #even if other columns are diff is salary matches it will drop the second one and onwards\n","\"\"\"\n","\n","#Dataframes**\n","\"\"\"\n","my_columns=[\"col1\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\", \"col8\", \"col9\", \"col10\",\"col11\",\"col12\"]\n","\n","example1=pd.read_csv('messy_employee_data.csv',names=my_columns, na_values='?')#does not fully replace names goes over and keeps original names at 0\n","print(example1.head())\n","\n","#note this replaces the column names entirely\n","example2=pd.read_csv('messy_employee_data.csv')\n","example2.columns = my_columns\n","\"\"\"\n","\n","#Series\n","\"\"\"\n","build df with Series obj  lile one dim array\n","\n","a=pd.Series([1,2,3,4,5,6,7,8,9])\n","b=pd.Series([\"One\",\"Two\",\"Three\",\"Four\",\"Five\",\"Six\",\"Seven\"])\n","c=pd.Series([0,1,0,1,0,1,0,1,0,1])\n","newdata=pd.DataFrame({\"column1\":a,\"column2\":b}) #use .DataFrame\n","\n","gives:\n","\n","  column1 column2\n","0        1     One\n","1        2     Two\n","2        3   Three\n","3        4    Four\n","4        5    Five\n","5        6     Six\n","6        7   Seven\n","7        8     NaN\n","8        9     NaN\n","\n","\n","\"\"\"\n","#Changing Data Types\n","\"\"\"\n","data_1['Salary'] = data_1['Salary'].astype('int')\n","\"\"\"\n","\n","#String Manipulation\n","\"\"\"\n","import re\n","data_1['year'] = data_1['Date_Joined'].astype(str).str.extract(r'(\\d{4})')  # Extract year from a date string, there are other ways to do this as it was already a datatime object\n","#Example 3: Replacing Values\n","#data_1['Gender'] = data_1['Gender'].replace({'male': 'M','Male': 'M', 'female': 'F','Female': 'F'})\n","data_1['Gender'] = data_1['Gender'].replace({re.compile('[mM]ale'): 'M',re.compile('[fF]emale'): 'F'})\n","\n","\"\"\"\n","#Merging Datasets\n","\"\"\"\n","-inner merge (default)\n","keeps rows where ID exists in both\n","inner_merged = pd.merge(df1, df2, on='ID')\n","\n","\n","-outer merge\n","keeps all rows from both dataframes\n","outer_merged = pd.merge(df1, df2, on='ID', how='outer') make sure to say how\n","\n","\n","-left merge\n","left_merged = pd.merge(df1, df2, on='ID', how='left')\n","Keeps all rows from the left DataFrame (df1).\n","\n","Matches from df2, fills NaN if no match.\n","\n","\n","\n","\n","-right merge\n","Keeps all rows from the right DataFrame (df2).\n","\n","Matches from df1, fills NaN if no match.\n","\n","\n","\"\"\"\n","#Dataframe Transofrmations\n","\"\"\"\n","pivot() to reshape df\n","df.pivot(index, columns, values) index acts as x , columns as 'by', values as y\n","\n","#good when graphing^^\n","\n","\n","# .agg() and .groupby()\n","\n","df.groupby(['A'])['C'].agg(['mean']) #we are groupby on A and for that we want mean of C\n","df.groupby('A')['C'].agg(['mean', 'sum']) #multiple fucntions\n","\n","df.groupby('A')['C'].mean() #can also do without .agg()\n","\n","when dealing with duplicates u can use .pivot_table() and u can have fucntions in it:\n","data.pivot_table(index='Department', columns='Month', values='Salary', aggfunc='mean')\n","\n","having multiple fucntions syntax :\n","print(tipsdata.groupby(['sex','smoker','size'])['tip'].agg(['mean'])),((['max'])),((['min']))\n","\n","\n","\"\"\"\n","\n","\n"],"metadata":{"id":"mHe8CzYrYEfY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1. Practice"],"metadata":{"id":"DUryt76bKx8u"}},{"cell_type":"code","source":["##Conceptual:\n","\"\"\"\n","Q1. What is the difference between cleaning, transforming, and feature engineering in data preparation?\n","\n","-cleaning is handling inconsistencies in the data, whereas transforming is turning the data in the specifc formt you want\n","and making it a dataframe or some object. Feauture engineering is changing some features or doing things on certain columns\n","\n","Q2. What is the difference between a DataFrame and a Series in pandas?\n","- Dataframe is a table like structure whereas a Series is a Series object\n","which is like a one dimensional array\n","\n","\n","Q3. If you want to drop all columns that have fewer than 5 non-null values, which command would you use?\n","- You would use dara.dropna(thresh=5, axis=1) which would keep if the row had atleast 5 non null values\n","\n","\n","Q4. What is the purpose of using subset in dropna()?\n","- subset in dropna() can be used to specify which columns to consider for dropping rows\n","\n","Q5. Fill in the blank:\n","If I want to fill all missing numeric values with zero in specific columns ['salary', 'bonus'], I should write:\n","- data_filled=data[['salary', 'bonus']].fillna(0) #doubke brackets for multiple columns\n","\n","Q6. What’s the difference between .ffill() and .bfill()? Also, how would you apply it across columns?\n","- .ffil() works by filling the df vertically from top to bottom and uses the preceding value,\n","while .bfill() goes bottom to top and uses succeeding value\n","\n","\n","Q7. How would you drop duplicate rows based only on the 'Salary' column, even if other columns differ?\n","-data=data.drop_duplicates(subset=['Salary'])\n","\n","Q8. You’re reading a messy CSV file, and you want to fully replace the column names with your own list of names.\n","Which method should you use after reading the file?\n","\n","mycols=['col1', 'col2']\n","data=pd.read_csv('filename')\n","data.columns=mycols\n","\n","Q9. How would you build a DataFrame using pandas Series objects for columns called 'column1' and 'column2'?\n","a=pd.Series([1,2,3])\n","b=pd.Series(['A', 'B;, 'C'])\n","data=pd.DataFrame({'column1':a, 'column2':b})\n","\n","\n","Q10. Write the line of code to convert the 'Salary' column to integers.\n","- data=data['Salary'].astype(int)\n","\n","\n","**Q11. If the 'Date_Joined' column contains dates as strings, how would you extract the year using regex?\n","import re\n","data['year']=data['Date_Joined'].astype(str).str.extract(r'(\\d{4})')\n","\n","\n","**Q12. Write the line of code to standardize the values in the 'Gender' column to 'M' and 'F' using regex.\n","data['Gender']=data['Gender'].replace({\n","  re.compile(r'[mM]ale): 'M',\n","  re.complie(r'[fF]emale): 'F'\n","})]\n","\n","\n","Q13. What is the difference between inner, outer, left, and right merges?\n","\n","-inner merge which is default merges two df and works as an 'and' only whats in both df's\n","-outer is the opposite acts like an 'or' and merges all rows and columns based on whats specifed\n","- left: merges from the first df\n","-right: merges from the right df\n","\n","\n","Q14. If you want to keep all rows from the right DataFrame, and match data from the left where possible, which type of merge would you use?\n","-you would use right merege\n","\n","Q15. What is the difference between pivot() and pivot_table()? Why might you prefer pivot_table()?\n","-pivot() is used to adjust certain columns and values in a df into a table with values and index and columns\n","-pivot_table() is better with handling duplicates and u can add agg fucntions in it\n","\n","Q16. Write the code to group a DataFrame by column 'A' and calculate both the mean and sum of column 'C'.\n","- data.groupby('A')['C'].agg(['mean', 'sum']) #pass like a list\n","\n","Q17. True or False:\n","You must always use .agg() to calculate the mean after groupby().\n","False\n","\n","Q18. How would you group by multiple columns, like 'sex', 'smoker', and 'size', and get the mean, max, and min of 'tip'?\n","data.groupby(['sex', 'smoker', 'size']).agg(['mean', 'max', 'min'])\n","\n","\n","\n","\"\"\"\n","#More Hands-On\n","\"\"\"\n","1. You have a DataFrame with many missing values across different columns.\n","Write the code to: a) Drop all rows where both 'Age' and 'Salary' are missing.\n","b) For numeric columns, fill missing values with zero.\n","\n","\n","a) data=data.dropna(subset=['Age', 'Salary']])\n","**b)\n","numeric=data.select_dtypes(exclude=object)\n","data[numeric]=data[numeric].fillna(0)\n","\n","\n","2. If you want to drop columns that have fewer than 3 non-null values, what would you write?\n","data=data.dropna(thresh=3, axis=1)\n","\n","\n","**3. Write the code to: a) Forward fill missing values across rows.\n","b) Backward fill missing values down the columns.\n","\n","a) data.ffil()\n","b) data.bfill(axis=1)\n","\n","\n","**4. Given a DataFrame with columns 'Employee_ID', 'Salary', and 'Department',\n","write the code to remove duplicate entries in 'Employee_ID' while keeping the first occurrence.\n","data=data.drop_duplicates(subset='Employee_ID) #assign to data because ur dropping from whole df based on that column\n","\n","\n","5.\n","df = pd.DataFrame({\n","    'Region': ['East', 'West', 'East', 'West', 'East'],\n","    'Sales': [200, 150, 300, 100, 250],\n","    'Profit': [20, 15, 30, 10, 25]\n","})\n","a) Group by 'Region' and calculate the total sales.\n","df.groupby('Region')['Sales'].sum()\n","\n","b) Group by 'Region' and calculate both total sales and total profit.\n","df.groupby('Region')[['Sales', 'Profit']].agg(['sum'])\n","\n","\n","\n","\n","6.\n","df1 = pd.DataFrame({'ID': [1, 2, 3], 'Name': ['Alice', 'Bob', 'Charlie']})\n","df2 = pd.DataFrame({'ID': [2, 3, 4], 'Score': [85, 90, 95]})\n","\n","a) Write the code for an inner merge.\n","pd.merge(df1,df2)\n","\n","\n","b) Write the code for an outer merge.\n","pd.merge(df1,df2,how='outer')\n","\n","\n","\n","7. df = pd.DataFrame({\n","    'Department': ['HR', 'HR', 'IT', 'IT', 'Finance'],\n","    'Month': ['Jan', 'Feb', 'Jan', 'Feb', 'Jan'],\n","    'Salary': [4000, 4500, 5000, 5200, 4800]\n","})\n","Create a pivot table showing average salary per department and month.\n","df.pivot_table(index='Department', columns='Month', values='Salary', aggfunc='mean')\n","#make sure to use aggfunc\n","\n","\n","8. Given a column 'Price' in your DataFrame, write the code to convert it to float type.\n","data['Price']=data['Price'].astype(float)\n","\n","\"\"\"\n","\n","#regex review:\n","\"\"\"\n","\n","1. You have a column 'Employee_ID' like this:\n","\n","EMP-123\n","EMP-456\n","EMP-789\n"," Task: Extract only the numeric part (e.g., 123, 456, etc.) and save it in a new column called 'Emp_Number'\n","\n","data['Emp_Number']=data['Employee_ID'].str.extract(r'\\d{3}')\n","\n","\n","2.\n","data['Month']=data['Date'].str.extract(r'-(\\d{2})-') str.extract gets the first occurence\n","\n","3.\n","You have an 'Email' column like:\n","\n","john.doe@example.com\n","jane_smith@gmail.com\n","user123@yahoo.com\n","Task: Extract everything before the '@' symbol and store it in a new column 'Username'.\n","\n","data['Username']=data['Email'].str.extract(r'([a-z._0-9]+)@') or (r'([^@]+)@') any char execpt @\n","\n","\n","**4. You have a 'Department' column with inconsistent cases:\n","'hr', 'HR', 'Hr', 'hR'\n","Task: Standardize all variations to 'HR' using regex.\n","\n","- .replace({pattern:replacement})\n","\n","data['Department']=data['Department'].replace({re.complie([hH][rR]): 'HR'})\n","\n","\n","5. (Bonus) You have a 'PhoneNumber' column with values like:\n","\n","'(123) 456-7890'\n","'123-456-7890'\n","'1234567890'\n","Task: Extract the 10-digit number and save it in uniform format: '1234567890'.\n","\n","data['Clean_Phone'] = data['PhoneNumber'].str.replace(r'\\D', '', regex=True)\n","- use replace to replace any non digit with empty space\n","\"\"\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"id":"5PdwSTDDK00b","executionInfo":{"status":"ok","timestamp":1743999151682,"user_tz":420,"elapsed":15,"user":{"displayName":"Samriddhi Matharu","userId":"15444417249591811102"}},"outputId":"945c42d5-4c39-479c-adcc-efd7689a99b5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n\\n1. You have a column 'Employee_ID' like this:\\n\\nEMP-123  \\nEMP-456  \\nEMP-789\\n Task: Extract only the numeric part (e.g., 123, 456, etc.) and save it in a new column called 'Emp_Number'\\n\\ndata['Emp_Number']=data['Employee_ID'].str.extract(r'\\\\d{3}')\\n\\n\\n2. \\ndata['Month']=data['Date'].str.extract(r'-(\\\\d{2})-') str.extract gets the first occurence\\n\\n3. \\nYou have an 'Email' column like:\\n\\njohn.doe@example.com\\njane_smith@gmail.com\\nuser123@yahoo.com\\nTask: Extract everything before the '@' symbol and store it in a new column 'Username'.\\n\\ndata['Username']=data['Email'].str.extract(r'([a-z._0-9]+)@') or (r'([^@]+)@') any char execpt @\\n\\n\\n**4. You have a 'Department' column with inconsistent cases:\\n'hr', 'HR', 'Hr', 'hR'\\nTask: Standardize all variations to 'HR' using regex.\\n\\n- .replace({pattern:replacement})\\n\\ndata['Department']=data['Department'].replace({re.complie([hH][rR]): 'HR'})\\n\\n\\n5. (Bonus) You have a 'PhoneNumber' column with values like:\\n\\n'(123) 456-7890'\\n'123-456-7890'\\n'1234567890'\\nTask: Extract the 10-digit number and save it in uniform format: '1234567890'.\\n\\ndata['Clean_Phone'] = data['PhoneNumber'].str.replace(r'\\\\D', '', regex=True)\\n- use replace to replace any non digit with empty space\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["2. Data Wrangling"],"metadata":{"id":"48XXKdwr46FI"}},{"cell_type":"code","source":["#melt, stack and unstack\n","\n","\"\"\"\n","melt() used to transform data from wide to long\n","coverting rows to columns, good for visuals (reshaping)\n","\n","syntax: df.melt(id_vars='',value_vars='', var_name='', value_name='')\n","id_vars=columns to stay same\n","value_vars=columns to melt into rows\n","var_name=name of columns containing value_vars\n","value_name= name of column containing corresponding values of value_vars\n","\n","ex) # Melt the DataFrame\n","melted_df = df.melt(id_vars=['FName','LName'], value_vars=[\"Math\",\"Chem\"],var_name='Subject', value_name='Score')\n","print(\"\\nMelted DataFrame:\")\n","\n","Original DataFrame:\n","  FName LName  Math  Chem  CS  Phy\n","0     A     X     1    88  50   78\n","1     B     Y     2    95  55   85\n","2     C     Z     3    80  60   90\n","\n","Melted DataFrame:\n","  FName LName Subject  Score\n","0     A     X    Math      1\n","1     B     Y    Math      2\n","2     C     Z    Math      3\n","3     A     X    Chem     88\n","4     B     Y    Chem     95\n","5     C     Z    Chem     80\n","\n","\"\"\"\n","#sentiment analysis:\n","\"\"\"\n","1. VADER(rile-based, tuned for social media)\n","2. TextBlob (simple library)\n","3. Hugging Face Tranformations (DL models)\n","4. Flair (more exploration)\n","\n","\n","1.\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","import nltk\n","nltk.download('vader_lexicon')\n","\n","# Initialize VADER\n","vader = SentimentIntensityAnalyzer()\n","\n","\n","# Example sentence for individual scoring\n","sentence = \"I absolutely loved the movie, though some parts were a bit slow.\"\n","scores = vader.polarity_scores(sentence)\n","print(\"VADER Scores:\", scores)\n","\n","# Function to apply to a DataFrame column\n","def vader_sentiment(text):\n","    return vader.polarity_scores(text)['compound'] #compund gives the final overall score\n","\n","movie_data['vader_sentiment'] = movie_data['text'].apply(vader_sentiment)\n","print(movie_data.iloc[0]['text'])  # integer-location , gets the first row for both the text and score\n","print(movie_data.iloc[0]['vader_sentiment'])\n","\n","____________________________________________________________________________\n","\n","2. TextBlob\n","\n","from textblob import TextBlob\n","\n","# Example using TextBlob\n","review = \"The movie was fantastic and inspiring, but the ending was disappointing.\"\n","blob = TextBlob(review)\n","print(\"TextBlob Polarity:\", blob.sentiment.polarity)\n","print(\"TextBlob Subjectivity:\", blob.sentiment.subjectivity)\n","\n","# Apply TextBlob sentiment to Twitter data\n","tweet_data['textblob_sentiment'] = tweet_data['tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n","print(tweet_data[['tweet', 'textblob_sentiment']])\n","\n","____________________________________________________________________________\n","3. Hugging Face\n","\n","from transformers import pipeline\n","\n","# Create sentiment analysis pipeline\n","sentiment_pipeline = pipeline(\"sentiment-analysis\",model=\"facebook/bart-large-mnli\")\n","#not all models are free. bert, bart, gpt2 and some others are\n","# Example sentence\n","result = sentiment_pipeline(\"I was not impressed by the movie; it felt outdated and dull.\")\n","print(\"Transformers Result:\", result)\n","\n","______________________________________________________________________________\n","4. Flair\n","\n","# Uncomment and install flair if desired: !pip install flair\n","!pip install flair\n","from flair.models import TextClassifier\n","from flair.data import Sentence\n","\n","# Load the sentiment classifier from Flair\n","classifier = TextClassifier.load('en-sentiment')\n","sentence = Sentence(\"The service at the theater was outstanding, yet the movie fell flat.\")\n","classifier.predict(sentence)\n","print(\"Flair Sentiment:\", sentence.labels)\n","\n","\n","\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"id":"QVfPVzBtK1US","executionInfo":{"status":"ok","timestamp":1743999154899,"user_tz":420,"elapsed":7,"user":{"displayName":"Samriddhi Matharu","userId":"15444417249591811102"}},"outputId":"b189121a-e17e-4094-f35d-17954afa2faf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n1. VADER(rile-based, tuned for social media)\\n2. TextBlob (simple library)\\n3. Hugging Face Tranformations (DL models)\\n4. Flair (more exploration)\\n\\n\\n1. \\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\\nimport nltk\\nnltk.download(\\'vader_lexicon\\')\\n\\n# Initialize VADER\\nvader = SentimentIntensityAnalyzer()\\n\\n\\n# Example sentence for individual scoring\\nsentence = \"I absolutely loved the movie, though some parts were a bit slow.\"\\nscores = vader.polarity_scores(sentence)\\nprint(\"VADER Scores:\", scores)\\n\\n# Function to apply to a DataFrame column\\ndef vader_sentiment(text):\\n    return vader.polarity_scores(text)[\\'compound\\'] #compund gives the final overall score \\n\\nmovie_data[\\'vader_sentiment\\'] = movie_data[\\'text\\'].apply(vader_sentiment)\\nprint(movie_data.iloc[0][\\'text\\'])  # integer-location , gets the first row for both the text and score \\nprint(movie_data.iloc[0][\\'vader_sentiment\\'])\\n\\n____________________________________________________________________________\\n\\n2. TextBlob\\n\\nfrom textblob import TextBlob\\n\\n# Example using TextBlob\\nreview = \"The movie was fantastic and inspiring, but the ending was disappointing.\"\\nblob = TextBlob(review)\\nprint(\"TextBlob Polarity:\", blob.sentiment.polarity)\\nprint(\"TextBlob Subjectivity:\", blob.sentiment.subjectivity)\\n\\n# Apply TextBlob sentiment to Twitter data\\ntweet_data[\\'textblob_sentiment\\'] = tweet_data[\\'tweet\\'].apply(lambda x: TextBlob(x).sentiment.polarity)\\nprint(tweet_data[[\\'tweet\\', \\'textblob_sentiment\\']])\\n\\n____________________________________________________________________________\\n3. Hugging Face\\n\\nfrom transformers import pipeline\\n\\n# Create sentiment analysis pipeline\\nsentiment_pipeline = pipeline(\"sentiment-analysis\",model=\"facebook/bart-large-mnli\")\\n#not all models are free. bert, bart, gpt2 and some others are\\n# Example sentence\\nresult = sentiment_pipeline(\"I was not impressed by the movie; it felt outdated and dull.\")\\nprint(\"Transformers Result:\", result)\\n\\n______________________________________________________________________________\\n4. Flair\\n\\n# Uncomment and install flair if desired: !pip install flair\\n!pip install flair\\nfrom flair.models import TextClassifier\\nfrom flair.data import Sentence\\n\\n# Load the sentiment classifier from Flair\\nclassifier = TextClassifier.load(\\'en-sentiment\\')\\nsentence = Sentence(\"The service at the theater was outstanding, yet the movie fell flat.\")\\nclassifier.predict(sentence)\\nprint(\"Flair Sentiment:\", sentence.labels)\\n\\n\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# conceptual plus hands on\n","\"\"\"\n","\n","Q1.When would you use .melt()? Can you give me a scenario (example dataset type or visual task)?\n","- .melt() would be used when u need to reshape the data for visualization purposes.\n","\n","ex) i have a dataframe called df\n","df.melt(id_vars, value_vars, var_name, value_name)\n","\n","\n","Q2.What is the purpose of id_vars in the melt function?\n","- id_vars is what u want to keep as columns and not melt\n","\n","\n","Q3.Suppose you have this wide DataFrame:\n","\n","Name  Math  Science\n","A     90    85\n","B     80    88\n","What will happen if you do:\n","\n","df.melt(id_vars='Name', value_vars=['Math', 'Science'], var_name='Subject', value_name='Score')\n","\n","Describe what the output will look like (not exact syntax, just shape/columns).\n","- name wil remian as a column, while math and science will reshape into rows. The name of the column with math and scinece will be subject,\n","and the next column with values will be the scores with the label score\n","\n","\n","Q4. What is the difference between VADER and TextBlob? (Give me one advantage of each.)\n","- vader is a more tuned for social media while textblob is a more basic text analysis tool\n","\n","Q5. For VADER, let’s say you have the compound score for a sentence. How would you interpret these scores:\n","\n","Compound score: 0.8\n","- This score has a pretty strng positive sentiment score\n","\n","Compound score: 0.0\n","- There is no sentiment\n","\n","Compound score: -0.5\n","- There is mpderate negtiave sentiment\n","\n","\"\"\"\n","#More hands on:\n","\n","#vader\n","\"\"\"\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","import nltk\n","nltk.download('vader_lexicon')\n","vader = SentimentIntensityAnalyzer()\n","\n","\n","Task 1: Analyze a simple sentence\n","Sentence: \"The food was amazing but the service was terrible.\"\n","Write the code to get the polarity scores dictionary.\n","\n","-\n","scores=vader.polarity_scores(sentence)\n","print('The scores are: {scores})\n","\n","\n","Task 2: Get only the compound score\n"," Using the same sentence, extract just the compound score.\n","- final_score=vader.polarity_scores(sentence)['compound']\n","\n","\n","Task 3: Apply VADER to a pandas DataFrame\n","Imagine you have:\n","\n","import pandas as pd\n","reviews = pd.DataFrame({'review': [\"The movie was fantastic!\", \"It was okay, not great.\", \"Worst experience ever.\"]})\n"," Write the code to apply VADER and create a new column called 'vader_compound' containing the compound scores.\n","- reviews['vader_compound']=reviews['review'].apply(lambda x: vader.polarity_scores(x)['compound'])\n","print(reviews)\n","\n","\n","**Task 4: Using the compound score in your DataFrame, create a new column called 'vader_sentiment' with labels:\n","Positive (compound > 0.05)\n","Neutral (between -0.05 and 0.05)\n","Negative (compound < -0.05)\n","\n","\n","def classify_sentiment(score):\n","    if score > 0.05:\n","        return 'Positive'\n","    elif score < -0.05:\n","        return 'Negative'\n","    else:\n","        return 'Neutral'\n","\n","reviews['vader_sentiment']=reviews['vader_compound'].apply(classify_sentiment)\n","\n","\"\"\"\n","#textblob\n","\"\"\"\n","Task 1: Analyze a sentence with TextBlob\n","\n","\"The movie had stunning visuals but the story was lacking depth.\"\n"," Use TextBlob to print: Polarity, Subjectivity\n","\n","from textblob import TextBlob\n","sentence=\"The movie had stunning visuals but the story was lacking depth.\"\n","\n","blob=TextBlob(sentence)\n","print(blob.sentiment) #gives polarity and subjectivity\n","\n","\n","\n","\n","**Task 2: Apply TextBlob to a DataFrame\n","Sample DataFrame:\n","\n","import pandas as pd\n","reviews = pd.DataFrame({'review': [\"Amazing performance!\", \"Not worth the time.\", \"It was decent, could be better.\"]})\n","Apply TextBlob to get the polarity score in a new column called 'textblob_polarity'.\n","\n","-reviews['textblob_polarity']=reviews['review'].apply(lambda x: TextBlob(x).sentiment.polarity)\n","\n","\n","Task 3: Classify sentiment from polarity score\n","Add a new column 'textblob_sentiment':\n","Positive if polarity > 0\n","Neutral if polarity == 0\n","Negative if polarity < 0\n","\n","def classify_sentiment(polarity):\n","  if polarity>0:\n","    return 'Positive'\n","  elif polarity<0:\n","    return 'Negative'\n","  else:\n","    return 'Neutral'\n","\n","-reviews['textblob_sentiment']=reviews['textblob_polarity'].apply(classify_sentiment)\n","\n","Bonus Task 4 (Optional):\n","Can you also add the subjectivity score in a new column called 'textblob_subjectivity'\n","-\n","def subjectivity(text):\n","  return TextBlob(text).sentiment.subjectivity)\n","\n","reviews['textblob_subjectivity']=reviews['review'].apply(subjectivity)\n","\n","\n","\"\"\"\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"VNG8IA_mK1Qw","executionInfo":{"status":"ok","timestamp":1743999158036,"user_tz":420,"elapsed":6,"user":{"displayName":"Samriddhi Matharu","userId":"15444417249591811102"}},"outputId":"c738a897-5d66-4af9-f12d-ca2bee2d2d3f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\nQ1.When would you use .melt()? Can you give me a scenario (example dataset type or visual task)?\\n- .melt() would be used when u need to reshape the data for visualization purposes. \\n\\nex) i have a dataframe called df \\ndf.melt(id_vars, value_vars, var_name, value_name)\\n\\n\\n\\n\\n\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["#more:\n","\"\"\"\n","\n","data = pd.DataFrame({\n","    'Department': ['HR', 'IT', 'Finance', 'HR', 'IT'],\n","    'Salary': [4000, 5000, 5500, 4200, 4800],\n","    'Experience': [2, 5, 7, 3, 4]\n","})\n","✅ Task:\n","Group by 'Department' and calculate:\n","\n","Mean salary\n","\n","Max experience\n","\n","Write the code!\n","\n","\n","mean_salary=data.groupby('Department')['Salary'].mean()\n","max_exp=data.groupby('Department')['Experience'].max()\n","\n","OR\n","\n","department_stats = data.groupby('Department').agg({'Salary': 'mean', 'Experience': 'max'})\n","print(department_stats)\n","\n","\n","**✅ Bonus:\n","Assign the mean salary back as a new column to the original DataFrame (so every row in 'HR' gets its department’s mean salary).\n","data['mean_sal']=data['Department'].map(mean_salary)\n","\n","\n","\n","\n","\n","Q3.\n","Given:\n","df = pd.DataFrame({\n","    'FName': ['A', 'B'],\n","    'LName': ['X', 'Y'],\n","    'Math': [80, 90],\n","    'Science': [85, 95]\n","})\n","✅ Task:\n","Use melt() to reshape the DataFrame so that:\n","\n","'FName' and 'LName' stay as they are.\n","\n","'Math' and 'Science' become a new column 'Subject'.\n","\n","Their scores go in a column 'Score'.\n","\n","\n","data=data.melt(id_vars=['FName', 'LName'],value_vars=['Math', 'Science'], var_name='Subject', value_name='Score')\n","\n","\n","\n","Q4.\n","Given:\n","\n","data = pd.DataFrame({\n","    'Date_Joined': ['2021-05-14', '2020-08-20', '2019-07-11']\n","})\n","✅ Task:\n","Extract the year from the 'Date_Joined' column using regex.\n","\n","\n","import re\n","year=data['Date Joined'].str.extract(r'(\\d{4})')\n","\n","\n","\n","Q5.\n","Given:\n","\n","data = pd.DataFrame({\n","    'Email': ['john.doe@example.com', 'jane_smith@work.org']\n","})\n","✅ Task:\n","Extract just the username part (before the @) from the email addresses using regex.\n","\n","import re\n","preuser=data['Email'].str.extract(r'([a-zA-Z0-9._%+-]+)@')\n","\n","\n","\n","\n","Q6. (Bonus regex)\n","Given:\n","data = pd.DataFrame({\n","    'Department': ['hr', 'HR', 'It', 'it', 'finance', 'Finance']\n","})\n","✅ Task:\n","Standardize 'Department' to be:\n","\n","'HR' for any case of 'hr'\n","\n","'IT' for any case of 'it'\n","\n","'Finance' for any case of 'finance'\n","\n","data['Department']=data['Department'].replace({\n","  re.compile('hr'): 'HR', re.compile('it'):'IT', re.compile('finance'): 'Finance'})\n","\n","\n","\n","\"\"\"\n","\n"],"metadata":{"id":"ve-mgZpS4VqM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Cheat sheet:\n","\"\"\"\n","#dataframes\n","- data={'review_id': [1,2,3], ...}\n","\n","cleaning: removing errors or inconsistencies\n","transforming: chnaging dtypes, values or structures\n","filtering: selecting relevant data points\n","feature engineeering: creating new feautures from existing data\n","\n","\n","\n","#cleaning/manpulation\n","data.dropna()\n","df.fillna()\n","df.groupby()\n","\n","\n","\n","data=data.dropna(subset=['col1', 'col2']) drops missing rows from those columns\n","data2=data.dropna(axis=1) axis 0 is rows (defualt), 1 is columns\n","\n","\n","thresh=data.dropna(thresh=10, axis=1) --> for columns\n","\n","\n","\n","data_filled=data[['salary', 'bonus', 'age']].fillna(0)\n","data_filled=data.fillna({'First Name': 'Unknown', 'Date_Joined': 'Unknown'}) or numerical types\n","\n","numeric=data.select_dtypes(exclude='object').columns #gets only numeric\n","data_mean=data.fillna(data[numeric]).mean())\n","\n","\n","data_fill=data.ffil() #takes PRECEDING (top to bottom)\n","data_fill=data.bfill() #takes SUCCEDEING val (bottom to top)\n","\n","\n","data=data.drop_duplicates(subset=['Salary']) #even if other columns are diff is salary matches it will drop the second one and onwards\n","\n","\n","#series\n","\n","a=pd.Series([1,2,3,4,5,6,7,8,9])\n","newdata=pd.DataFrame({\"column1\":a,\"column2\":b}) #use .DataFrame\n","\n","\n","#string manipualtion\n","import re\n","data_1['year'] = data_1['Date_Joined'].astype(str).str.extract(r'(\\d{4})')  # Extract year from a date string, there are other ways to do this as it was already a datatime object\n","#Example 3: Replacing Values\n","#data_1['Gender'] = data_1['Gender'].replace({'male': 'M','Male': 'M', 'female': 'F','Female': 'F'})\n","data_1['Gender'] = data_1['Gender'].replace({re.compile('[mM]ale'): 'M',re.compile('[fF]emale'): 'F'})\n","\n","\n","\n","\n","#merge\n","inner merge (default)\n","keeps rows where ID exists in both\n","inner_merged = pd.merge(df1, df2, on='ID')\n","\n","\n","rest is , outer(keep all rows from df), left(all rows fro left df and tries tro matcch, if no match then Nan)\n","\n","\n","\n","\n","#tranfrmatrions:\n","#Dataframe Transofrmations\n","\n","pivot() to reshape df\n","df.pivot(index, columns, values) index acts as x , columns as 'by', values as y\n","\n","#good when graphing^^\n","\n","\n","# .agg() and .groupby()\n","\n","df.groupby('A')['C'].agg(['mean', 'sum']) #multiple fucntions\n","\n","when dealing with duplicates u can use .pivot_table() and u can have fucntions in it:\n","data.pivot_table(index='Department', columns='Month', values='Salary', aggfunc='mean')\n","\n","having multiple fucntions syntax :\n","tipsdata.groupby(['sex', 'smoker', 'size'])['tip'].agg(['mean', 'max', 'min'])\n","\n","\n","#\n","**Q12. Write the line of code to standardize the values in the 'Gender' column to 'M' and 'F' using regex.\n","data['Gender']=data['Gender'].replace({\n","  re.compile(r'[mM]ale): 'M',\n","  re.complie(r'[fF]emale): 'F'\n","})]\n","\n","\n","1. You have a DataFrame with many missing values across different columns.\n","Write the code to: a) Drop all rows where both 'Age' and 'Salary' are missing.\n","b) For numeric columns, fill missing values with zero.\n","\n","\n","a) data=data.dropna(subset=['Age', 'Salary']])\n","**b)\n","numeric=data.select_dtypes(exclude=object)\n","data[numeric]=data[numeric].fillna(0)\n","\n","\n","\n","\n","#regex tips:\n","\n","#NOTE [^] this [] means match any character inside , ^ with it means negates that. So match any character NOT inside\n","# [^X]+ match everyhting UNTIL a certain char ex) [^@]+ match all until @\n","# (.*?) for BETWEN things so ex) \\[(.*?)\\] this is everyhting inside brackets\n","\n","2.\n","data['Month']=data['Date'].str.extract(r'-(\\d{2})-') str.extract gets the first occurence of this which is between - and -\n","\n","\n","data['Username']=data['Email'].str.extract(r'([a-z._0-9]+)@') or (r'([^@]+)@') any char execpt @\n","\n","\n","**4. You have a 'Department' column with inconsistent cases:\n","'hr', 'HR', 'Hr', 'hR'\n","Task: Standardize all variations to 'HR' using regex.\n","\n","- .replace({pattern:replacement})\n","\n","data['Department']=data['Department'].replace({re.complie([hH][rR]): 'HR'})\n","\n","\n","5. (Bonus) You have a 'PhoneNumber' column with values like:\n","\n","'(123) 456-7890'\n","'123-456-7890'\n","'1234567890'\n","Task: Extract the 10-digit number and save it in uniform format: '1234567890'.\n","\n","data['Clean_Phone'] = data['PhoneNumber'].str.replace(r'\\D', '', regex=True)\n","- use replace to replace any non digit with empty space\n","\n","\n","\n","\n","#mel stack, unstack:\n","syntax: df.melt(id_vars='',value_vars='', var_name='', value_name='')\n","id_vars=columns to stay same\n","value_vars=columns to melt into rows\n","var_name=name of columns containing value_vars\n","value_name= name of column containing corresponding values of value_vars\n","\n","ex) # Melt the DataFrame\n","melted_df = df.melt(id_vars=['FName','LName'], value_vars=[\"Math\",\"Chem\"],var_name='Subject', value_name='Score')\n","\n","\n","\n","\n","\n","\n","1. VADER(rile-based, tuned for social media)\n","2. TextBlob (simple library)\n","3. Hugging Face Tranformations (DL models)\n","4. Flair (more exploration)\n","\n","\n","1.\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","import nltk\n","nltk.download('vader_lexicon')\n","\n","# Initialize VADER\n","vader = SentimentIntensityAnalyzer()\n","\n","\n","# Example sentence for individual scoring\n","sentence = \"I absolutely loved the movie, though some parts were a bit slow.\"\n","scores = vader.polarity_scores(sentence)\n","print(\"VADER Scores:\", scores)\n","\n","# Function to apply to a DataFrame column\n","def vader_sentiment(text):\n","    return vader.polarity_scores(text)['compound'] #compund gives the final overall score\n","\n","movie_data['vader_sentiment'] = movie_data['text'].apply(vader_sentiment)\n","print(movie_data.iloc[0]['text'])  # integer-location , gets the first row for both the text and score\n","print(movie_data.iloc[0]['vader_sentiment'])\n","\n","____________________________________________________________________________\n","\n","2. TextBlob\n","\n","from textblob import TextBlob\n","\n","# Example using TextBlob\n","review = \"The movie was fantastic and inspiring, but the ending was disappointing.\"\n","blob = TextBlob(review)\n","print(\"TextBlob Polarity:\", blob.sentiment.polarity)\n","print(\"TextBlob Subjectivity:\", blob.sentiment.subjectivity)\n","\n","# Apply TextBlob sentiment to Twitter data\n","tweet_data['textblob_sentiment'] = tweet_data['tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n","print(tweet_data[['tweet', 'textblob_sentiment']])\n","\n","____________________________________________________________________________\n","3. Hugging Face\n","\n","from transformers import pipeline\n","\n","# Create sentiment analysis pipeline\n","sentiment_pipeline = pipeline(\"sentiment-analysis\",model=\"facebook/bart-large-mnli\")\n","#not all models are free. bert, bart, gpt2 and some others are\n","# Example sentence\n","result = sentiment_pipeline(\"I was not impressed by the movie; it felt outdated and dull.\")\n","print(\"Transformers Result:\", result)\n","\n","______________________________________________________________________________\n","4. Flair\n","\n","# Uncomment and install flair if desired: !pip install flair\n","!pip install flair\n","from flair.models import TextClassifier\n","from flair.data import Sentence\n","\n","# Load the sentiment classifier from Flair\n","classifier = TextClassifier.load('en-sentiment')\n","sentence = Sentence(\"The service at the theater was outstanding, yet the movie fell flat.\")\n","classifier.predict(sentence)\n","print(\"Flair Sentiment:\", sentence.labels)\n","\n","\n","\n","\n","\n","\n","#homework:\n","\n","\n","def clean(text):\n","\n","  hashtag_pattern=(r'#([\\w-]+)')\n","  mention_pattern=(r'@(\\w+)')\n","  emoji_pattern=r'[^\\w\\s,]'\n","  url_pattern=(r'http\\S+') #S is non white space chars\n","\n","  text=str(text)\n","\n","  text=re.sub(hashtag_pattern, r'\\1', text) #macth to whats inside forst parenthesis group\n","  text=re.sub(mention_pattern, r'\\1', text)\n","  text=re.sub(emoji_pattern, '', text)\n","  text=re.sub(url_pattern, '', text)\n","\n","  return text\n","\n","\n","data['cleaned_post_content']=data['post_content'].apply(clean)\n","\n","hashtag_pattern=(r'#([\\w-]+)')\n","data['Hashtags']=data['post_content'].str.findall(hashtag_pattern) #so u can use str.findall here\n","\n","\n","\n","top_users=averages.sort_values(by='total', ascending=False).head(3) #sort_values for dataframes\n","\n","\n","\n","post_sentiment=data['sentiment'].value_counts()\n","post_sentiment\n","\n","\n","\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","\n","all_hashtags=data['Hashtags'].dropna().explode() #drop null before exploding\n","hashtag_str=' '.join(all_hashtags) #join to one big string\n","\n","wc=WordCloud(width=800, height=400).generate(hashtag_str)\n","plt.figure(figsize=(10, 5))\n","plt.imshow(wc)\n","plt.axis('off')\n","plt.show()\n","\n","\n","\n","\n","\n","data.head()\n","data['year_month']=data['post_date'].dt.to_period('M').astype(str) #monthly period\n","post_per_month=data.groupby('year_month').size()\n","\n","x=post_per_month.index\n","y=post_per_month.values\n","plt.plot(x, y, marker='o', linestyle='-', color='b')\n","plt.xlabel('Months')\n","plt.xticks(rotation=45)\n","plt.ylabel('Post Count')\n","plt.title('Line Plot')\n","plt.show()\n","\n","\"\"\"\n","\n","\n"],"metadata":{"id":"anmKyUN7DOEO"},"execution_count":null,"outputs":[]}]}